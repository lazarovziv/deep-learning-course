{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a2047d5-e346-4400-bde9-0060099cbc9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T10:36:11.487300Z",
     "start_time": "2024-04-05T10:36:09.252925Z"
    },
    "id": "6a2047d5-e346-4400-bde9-0060099cbc9d"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Link to the explanation video:\n",
    "https://drive.google.com/file/d/18As9KEln4FgOReIXqjxmudeilEShhL1a/view?usp=sharing\n",
    "Last 3 minutes are explanations of the comparisons with PyTorch :)\n",
    "'''\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0415f0-49ac-4813-b6c0-4e1c8b320c28",
   "metadata": {
    "id": "ce0415f0-49ac-4813-b6c0-4e1c8b320c28"
   },
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13c347aa-1723-49b1-8148-10cbb1370fa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T10:36:19.898018Z",
     "start_time": "2024-04-05T10:36:19.879914Z"
    },
    "id": "13c347aa-1723-49b1-8148-10cbb1370fa3"
   },
   "outputs": [],
   "source": [
    "# defining the error for expand_as function\n",
    "class BroadcastError(Exception):\n",
    "    def __init__(self, a, b):\n",
    "        super().__init__(f'The tensors can\\'t be broadcasted together!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c985e-16d3-4bb3-99a8-0c83fce8178b",
   "metadata": {
    "id": "969c985e-16d3-4bb3-99a8-0c83fce8178b"
   },
   "source": [
    "### Q1.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36ea90f4-ca26-4f9b-a138-f11b7ea2c53b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T10:36:20.616491Z",
     "start_time": "2024-04-05T10:36:20.605023Z"
    },
    "id": "36ea90f4-ca26-4f9b-a138-f11b7ea2c53b"
   },
   "outputs": [],
   "source": [
    "def is_broadcastable(a, b):\n",
    "    smaller_dim = min(len(a.shape), len(b.shape))\n",
    "    bigger_dim = max(len(a.shape), len(b.shape))\n",
    "    is_a_bigger = len(a.shape) > len(b.shape)\n",
    "    expected_broadcasted_axes = []\n",
    "    # traversing the dimensions of the smaller tensor\n",
    "    for i in range(smaller_dim):\n",
    "        # i-th index from the end\n",
    "        idx = -1-i\n",
    "        # if the axes are different and none of them is a 1, they're not broadcastable\n",
    "        if a.shape[idx] != b.shape[idx] and (a.shape[idx] != 1 and b.shape[idx] != 1):\n",
    "            return False\n",
    "        expected_broadcasted_axes.append(max(a.shape[idx], b.shape[idx]))\n",
    "\n",
    "    # appending the expected axes values\n",
    "    for i in range(smaller_dim, bigger_dim):\n",
    "        idx = -1-i\n",
    "        if is_a_bigger:\n",
    "            expected_broadcasted_axes.append(a.shape[idx])\n",
    "        elif len(a.shape) == len(b.shape):\n",
    "            expected_broadcasted_axes.append(max(a.shape[idx], b.shape[idx]))\n",
    "        else:\n",
    "            expected_broadcasted_axes.append(b.shape[idx])\n",
    "    # returning the shape of the expected broadcasted tensor\n",
    "    return True, torch.Size(reversed(expected_broadcasted_axes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b71c738-1341-42d0-9c2f-9440a5321a44",
   "metadata": {
    "id": "3b71c738-1341-42d0-9c2f-9440a5321a44"
   },
   "source": [
    "### Q1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c1284d3-1f0d-4de8-b0f3-bb78cef94461",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T10:36:21.921880Z",
     "start_time": "2024-04-05T10:36:21.914400Z"
    },
    "id": "9c1284d3-1f0d-4de8-b0f3-bb78cef94461"
   },
   "outputs": [],
   "source": [
    "def expand_as(expand_from, expand_to):\n",
    "    a = expand_from\n",
    "    b = expand_to\n",
    "    # we can't broadcast if the tensor to be broadcasted is bigger than the target tensor\n",
    "    if len(a.shape) > len(b.shape) or not is_broadcastable(a, b):\n",
    "        raise BroadcastError(a, b)\n",
    "    # can't broadcast source to target where the i-th axis in source is bigger than i-th axis in target (rightmost)\n",
    "    for i in range(len(a.shape)):\n",
    "        idx = -1-i\n",
    "        if a.shape[idx] > b.shape[idx]:\n",
    "            raise BroadcastError(a, b)\n",
    "\n",
    "    dimensions_diff = len(b.shape) - len(a.shape)\n",
    "    # cloning the smaller tensor for building the broadcasted tensor\n",
    "    c = a.clone()\n",
    "\n",
    "    # adding axes to c\n",
    "    for i in range(dimensions_diff):\n",
    "        c = torch.unsqueeze(c, 0)\n",
    "\n",
    "    # making sure they're of the same length\n",
    "    assert len(c.shape) == len(b.shape)\n",
    "\n",
    "    other = b\n",
    "    c_axes, other_axes = list(c.shape), list(other.shape)\n",
    "\n",
    "    num_axes = len(c_axes)\n",
    "    # concatenating the shorter tensor to itself along the relevant axes\n",
    "    for i in range(num_axes):\n",
    "        idx = -1-i\n",
    "        current_axis = num_axes+idx\n",
    "        # if it's a 1, concatenate the current axis diff times\n",
    "        if c_axes[idx] < other_axes[idx]:\n",
    "            diff = other_axes[idx] - c_axes[idx]\n",
    "            # cloning c to be able to use it as it is now for the concatenation\n",
    "            cloned = c.clone()\n",
    "            for j in range(diff):\n",
    "                c = torch.cat((c, cloned), dim=current_axis)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d4dcf6-b5f9-4e90-8e6e-31564754dd28",
   "metadata": {
    "id": "88d4dcf6-b5f9-4e90-8e6e-31564754dd28"
   },
   "source": [
    "### Q1.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79142ecf-5664-4aed-be14-a40e3cedd206",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T10:36:22.984862Z",
     "start_time": "2024-04-05T10:36:22.975989Z"
    },
    "id": "79142ecf-5664-4aed-be14-a40e3cedd206"
   },
   "outputs": [],
   "source": [
    "def broadcast_tensors(a, b):\n",
    "    # if both can't be broadcasted to each other\n",
    "    if not is_broadcastable(a, b) and not is_broadcastable(b, a):\n",
    "        raise BroadcastError(a, b)\n",
    "\n",
    "    bigger_shape = a.shape if len(a.shape) > len(b.shape) else b.shape\n",
    "    smaller_shape = a.shape if len(a.shape) < len(b.shape) else b.shape\n",
    "    bigger_shape_length = max(len(a.shape), len(b.shape))\n",
    "    smaller_shape_length = min(len(a.shape), len(b.shape))\n",
    "\n",
    "    # getting the resulting tensor size\n",
    "    result_axes = []\n",
    "\n",
    "    for i in range(smaller_shape_length):\n",
    "        idx = -1-i\n",
    "        result_axes.append(max(a.shape[idx], b.shape[idx]))\n",
    "    # add the remaining axes\n",
    "    result_axes.extend(bigger_shape[:bigger_shape_length - smaller_shape_length])\n",
    "    # reverse the axes because we started from the lower axes for correct format\n",
    "    result_axes = result_axes[::-1]\n",
    "\n",
    "    # creating a tensor of the correct shape\n",
    "    tensor_with_final_shape = torch.zeros(result_axes)\n",
    "\n",
    "    # distinguishing between the 2 cases for intuitive usage of the function\n",
    "    # where calling the function with a, b we'll receive broadcasted_a, broadcasted_b\n",
    "    return expand_as(a, tensor_with_final_shape), expand_as(b, tensor_with_final_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q1.d"
   ],
   "metadata": {
    "id": "BJoz1bAZYU5c"
   },
   "id": "BJoz1bAZYU5c"
  },
  {
   "cell_type": "code",
   "source": [
    "tensors_pairs = [[torch.tensor([3, 3]), torch.tensor([2])],\n",
    "    [torch.tensor([[1,3,5],[1,3,5]]), torch.tensor([2])],\n",
    "    [torch.tensor([1,2]), torch.tensor([[2,3,4], [5,6,7]])],\n",
    "    [torch.arange(10**4).reshape(10, 10, 10, 1, 10), torch.arange(10**5).view(10, 10, 10, 10, 10)],\n",
    "    [torch.tensor([1,2,3]), torch.tensor([[2,3,4], [5,6,7]])],\n",
    "    [torch.tensor([[1,2,3]]), torch.tensor([[2,3,4], [5,6,7]])],\n",
    "\n",
    "    [torch.tensor([[[1,2,3]]]), torch.tensor([[2,3,4], [5,6,7]])],\n",
    "    [torch.arange(10**3).reshape(10, 1, 10, 1, 10), torch.arange(10**5).view(10, 10, 10, 10, 10)],\n",
    "    [torch.arange(10**3).reshape(10, 10, 1, 10), torch.arange(10**5).view(10, 10, 10, 10, 10)],\n",
    "    [torch.arange(10**2).reshape(10, 1, 1, 10), torch.arange(10**5).view(10, 10, 10, 10, 10)],\n",
    "\n",
    "    [torch.arange(10**2).reshape(10, 10), torch.arange(10**5).view(10, 10, 10, 10, 10)]]"
   ],
   "metadata": {
    "id": "15wzJheZCI8q"
   },
   "id": "15wzJheZCI8q",
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "number_of_examples = 3\n",
    "\n",
    "for i in range(number_of_examples):\n",
    "    a, b = tensors_pairs[i][0], tensors_pairs[i][1]\n",
    "    print(f'a - {a}', f'b - {b}', sep='\\n')\n",
    "    print(f'a.shape - {a.shape}', f'b.shape - {b.shape}', sep='\\n')\n",
    "    print()\n",
    "    print(f'Is broadcastable implementation: {is_broadcastable(a, b)}')\n",
    "    try:\n",
    "        print(f'expand_as implementation: {expand_as(a, b)}', f'PyTorch: {a.expand_as(b)}', sep='\\t')\n",
    "    except:\n",
    "        print('An error occured - expand_as(a,b)')\n",
    "    try:\n",
    "        print(f'expand_as implementation (reversed): {expand_as(b, a)}', f'PyTorch: {b.expand_as(a)}', sep='\\t')\n",
    "    except:\n",
    "        print('An error occured - expand_as(b,a)')\n",
    "    try:\n",
    "        print(f'Broadcast implementation: {broadcast_tensors(a, b)}', f'PyTorch: {torch.broadcast_tensors(a, b)}', sep='\\t')\n",
    "    except:\n",
    "        print('An error occured - broadcast_tensors(a,b)')\n",
    "\n",
    "    print('\\n\\n')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82n_V4ZHDnnE",
    "outputId": "6ee1f405-b02e-48ac-a175-c59466a8e0e3"
   },
   "id": "82n_V4ZHDnnE",
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a - tensor([3, 3])\n",
      "b - tensor([2])\n",
      "a.shape - torch.Size([2])\n",
      "b.shape - torch.Size([1])\n",
      "\n",
      "Is broadcastable implementation: (True, torch.Size([2]))\n",
      "An error occured - expand_as(a,b)\n",
      "expand_as implementation (reversed): tensor([2, 2])\tPyTorch: tensor([2, 2])\n",
      "Broadcast implementation: (tensor([3, 3]), tensor([2, 2]))\tPyTorch: (tensor([3, 3]), tensor([2, 2]))\n",
      "\n",
      "\n",
      "\n",
      "a - tensor([[1, 3, 5],\n",
      "        [1, 3, 5]])\n",
      "b - tensor([2])\n",
      "a.shape - torch.Size([2, 3])\n",
      "b.shape - torch.Size([1])\n",
      "\n",
      "Is broadcastable implementation: (True, torch.Size([2, 3]))\n",
      "An error occured - expand_as(a,b)\n",
      "expand_as implementation (reversed): tensor([[2, 2, 2],\n",
      "        [2, 2, 2]])\tPyTorch: tensor([[2, 2, 2],\n",
      "        [2, 2, 2]])\n",
      "Broadcast implementation: (tensor([[1, 3, 5],\n",
      "        [1, 3, 5]]), tensor([[2, 2, 2],\n",
      "        [2, 2, 2]]))\tPyTorch: (tensor([[1, 3, 5],\n",
      "        [1, 3, 5]]), tensor([[2, 2, 2],\n",
      "        [2, 2, 2]]))\n",
      "\n",
      "\n",
      "\n",
      "a - tensor([1, 2])\n",
      "b - tensor([[2, 3, 4],\n",
      "        [5, 6, 7]])\n",
      "a.shape - torch.Size([2])\n",
      "b.shape - torch.Size([2, 3])\n",
      "\n",
      "Is broadcastable implementation: False\n",
      "An error occured - expand_as(a,b)\n",
      "An error occured - expand_as(b,a)\n",
      "An error occured - broadcast_tensors(a,b)\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "WACRomUTabRQ"
   },
   "id": "WACRomUTabRQ",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
